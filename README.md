# awesome-online-courses [![Awesome](https://awesome.re/badge.svg)](https://awesome.re)

A repository of inspiring machine learning papers.

## Search and Recommendation
| Paper | Authors | Year| Code | Notes |
| --- | --- | --- | --- | --- |
|How Airbnb Tells You Will Enjoy Sunset Sailing in Barcelona? Recommendation in a Two-Sided Travel Marketplace|Liang Wu, Mihajlo Grobovic |2020 | | The paper presents Airbnb's efforts on building recommender system for two-sided maretplace. In particular, it expands the knowlege graph with location-specific concepts and utilize location features with global-local mode. |
|Embedding-based Retrieval in Facebook Search| Jui-Ting Huang, etc|2020 | | Many practical tips on how to make embedding based retrieval work, incl. training/serving|
|Personalized Flight Itinerary Ranking at Fliggy| Jinhong Huang, etc| 2020| | extracting user preferences from long-term booking behavior, real-time clicking behavior and behavior from group users with same intention, using a listwise feature encoding|

## Graphs
| Paper | Authors | Year | Code | Notes |
| --- | --- | --- | --- | --- |
|[When Does Self-Supervision Help Graph Convolutional Networks?](https://arxiv.org/abs/2006.09136)|Yuning You, etc| 2020| [Python](https://github.com/Shen-Lab/SS-GCNs)| This paper explores three schemas of incorporating self-supervision into GCNs: pretraining+finetuning, self-training, multi-task learning, and designs three self-supervision tasks on graph: node clustering, graph partitioning, and graph completion.|

## Anomaly Detection
| Paper | Authors | Year | Code | Notes |
| --- | --- | --- | --- | --- |
|[Learning Confidence for Out-of-Distribution Detection in Neural Networks](https://arxiv.org/abs/1802.04865)|Terrance DeVries, Graham W. Taylor|2018| [python](https://github.com/uoguelph-mlrg/confidence_estimation) | This paper proposes a method of estimating confidence for neural networks for out-of-distribution detection. A few important details: budget parameter, combating excessive regularization, and retraining misclassified examples, for the architecture to work well.|

## System
| Paper | Authors | Year| Code | Notes |
| --- | --- | --- | --- | --- |
|[fastai: A Layered API for Deep Learning](https://arxiv.org/abs/2002.04688)|Jeremy Howard, Sylvain Gugger|2020|[Github](https://github.com/fastai/fastai) | The paper documents the philosophy behind the API design of the popular DL library fast.ai, which provides low, mid, and high level APIs, offering productivity as well as flexibility |

## NLP
| Paper | Authors | Year | Code | Notes |
| --- | --- | --- | --- |
|Universal Language Model Fine-tuning for Text Classification| Jeremy Howard, Sebastian Ruder| 2018| |Techniques for fine-tuning: discriminative fine-tuning, slanted triangular learning rates, gradual unfreezing |


## Uncategorized for now
| Paper | Authors | Year | Code | Notes |
| --- | --- | --- | --- | --- |
|Enhancing The Reliability of Out-of-distribution Image Detection in Neural Networks|Shiyu Liang, Yixuan Li, R. Srikant| | |
|Beyond Sparsity: Tree Regularization of Deep Models for Interpretability| Mike Wu, Michael C. Hughes, Sonali Parbhoo, Maurizio Zazzi, Volker Roth, Finale Doshi-Velez| | | |
|Robust Spammer Detection by Nash Reinforcement Learning| | | | |
|Octet: Online Catalog Taxonomy Enrichment with Self-Supervision| | | | |
|DeepAR: Probabilistic Forecasting with Autoregressive Recurrent Networks| David Slinas, Valentin Flunkert, Jan Gasthaus| | | |
|Accelerating Large-Scale Inference with Anisotropic Vector Quantization| | | |[Google Blog](https://ai.googleblog.com/2020/07/announcing-scann-efficient-vector.html)| |
|Auto-Keras: An Efficient Neural Architecture Search System| | | | |
|When Do GNNs Work: Understanding and Improving Neighborhood Aggregation| | | | |
|Continuous Graph Neural Networks| | | | |
|TabNet: Attentive Interpretable Tabular Learning| | | | |
|StarSpace:Embed All The Things| | | | |
|CatBoost: gradient boosting with categorical features support| | | | |
|Beyond User Embedding Matrix: Learning to Hash for Modeling Large-Scale Users in Recommendations| | | | |
|On Calibration of Modern Neural Networks| | | | |
|Deep Neural Networks as Gaussian Processes| | | | |
|Learning and Transferring IDs Representation in E-commerce| | | | |
|AutoGluon-Tabular: Robust and Accurate AutoML for Structured Data| | | | |
|Training with Multi-Layer Embeddings for Model Reduction| | | | |
|Deep Learning Recommendation Model for Personalization and Recommendation Systems| | | | |
|On the Bottleneck of Graph Neural Networks and its Practical Implications| | | | |
|Neural Collaborative Filtering vs. Matrix Factorization Revisited| | | | |
|Neural Machine Translation of Rare Words with Subword Units| | | | |
|Debiasing Grid-based Product Search in E-commerce| | | | |
|Cyclical Learning Rates for Training Neural Networks| | | |[Blog](https://sgugger.github.io/how-do-you-find-a-good-learning-rate.html#how-do-you-find-a-good-learning-rate) |
|STRATEGIES FOR PRE-TRAINING GRAPH NEURAL NETWORKS| | | | |
|DistDGL: Distributed Graph Neural Network Training for Billion-Scale Graphs| | | | |
|Optimized Product Quantization for Approximate Nearest Neighbor Search| | | | |
|Hard-Aware Deeply Cascaded Embedding| | | | |
|Unbiased Learning-to-Rank with Biased Feedback| | | | |
|How to Fine-Tune BERT for Text Classification?| | | | |
|Self-training Improves Pre-training for Natural Language Understanding| | | | |
|Dynamic heterogeneous graph embedding using hierarchical attentions| | | | |
|Learning to Hash with Graph Neural Networks for Recommender Systems| | | | |
|GCC: Graph Contrastive Coding for Graph Neural Network Pre-Training| | | | |
|Graph-based Multi-hop Reasoning for Long Text Generation| | | | |
|AliGraph: A Comprehensive Graph Neural Network Platform| | | | |
|Language Models are Few-Shot Learners| | | | |
|Densely connected convolutional networks| | | | |
|Query Transformation for Multi-Lingual Product Search| | | | |
|TEM: Tree-enhanced Embedding Model for Explainable Recommendation| | | | |
|Follow-the-Regularized-Leader and Mirror Descent: Equivalence Theorems and L1 Regularization| | | | |
|Combining Label Propagation and Simple Models Out-performs Graph Neural Networks| | | | |
